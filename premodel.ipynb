{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In this notebook, we will be covering the entire premodeling phase\n",
    "Premodeling is the process of making further transformations to our data so that it is best-suited for training and evaluating a machine learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime \n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from joblib import dump, load\n",
    "from itertools import combinations\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_val_score, RepeatedKFold\n",
    "from sklearn.linear_model import Lasso, LogisticRegression\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import ensemble, tree, metrics\n",
    "\n",
    "# start where we stopped last time\n",
    "equity_data = pd.read_csv(\"C:/Users/lbianculli/dev/us_equities/python_processes/us_data_final2.csv\").drop(\"Unnamed: 0\", axis=1)\n",
    "equity_data[\"date_dt\"] = pd.to_datetime(equity_data[\"period\"], format='%Y-%m-%d')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function that will allow us to label our data. In this case, we are concerned with whether the next periods free cash flow will be in the top quantile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_deltas(df, feature, classification=True, current_period=\"2021-6\"):  \n",
    "    \"\"\" \n",
    "    Get period-over-period changes for a collection of features and return the updated DF \n",
    "    df: Pandas DataFrame of data inclusive of 'features' columns\n",
    "    features: columns of DataFrame for which individual deltas will be calculated\n",
    "    freq: Frequency of resampling period\n",
    "    classification: Whether this data is to be used for a classification problem\n",
    "    \"\"\"\n",
    "    feature_dfs = []\n",
    "    tmp = []\n",
    "    tickers = []\n",
    "    current_data = []\n",
    "    \n",
    "    feature_vals = pd.pivot_table(df, values=feature, index=\"date_dt\", columns=[\"ticker\"])  # pivot on specified feature\n",
    "\n",
    "    # we pivoted with tickers in columns. Now we iterate through them\n",
    "    for ticker in feature_vals.columns:\n",
    "        # Get just ticker, date, and feature\n",
    "        feature_series = feature_vals[ticker].dropna().reset_index()\n",
    "        feature_series.columns = [\"date\", f\"{feature}\"]\n",
    "\n",
    "        # downsample data to quarterly (as that is the fundamental data's frequency)\n",
    "        downsampled = feature_series.set_index(\"date\").resample(\"Q\").mean().reset_index()\n",
    "\n",
    "        # fill NaNs with mean and calculate next period's measure\n",
    "        downsampled = downsampled.fillna(value=downsampled[feature].mean())\n",
    "        downsampled[f\"next_{feature}\"] = downsampled[f\"{feature}\"].shift(-1)\n",
    "\n",
    "        # get year, month, day\n",
    "        downsampled[\"year\"] = downsampled[\"date\"].dt.year\n",
    "        downsampled[\"month\"] = downsampled[\"date\"].dt.month\n",
    "        downsampled[\"day\"] = downsampled[\"date\"].dt.day\n",
    "\n",
    "        # convert to strings for ID\n",
    "        downsampled[\"year\"] = downsampled[\"year\"].astype(str)\n",
    "        downsampled[\"month\"] = downsampled[\"month\"].astype(str)\n",
    "        downsampled[\"day\"] = downsampled[\"day\"].astype(str)\n",
    "        downsampled[\"period\"] = downsampled[\"year\"] + \"-\" + downsampled[\"month\"]\n",
    "\n",
    "        tickers.append([ticker] * downsampled.shape[0])\n",
    "\n",
    "        # append individual ticker data for given feature\n",
    "        tmp.append(downsampled)\n",
    "\n",
    "    # consildate feature data and calculate quantile\n",
    "    feature_data = pd.concat([df for df in tmp], axis=0).reset_index(drop=True)\n",
    "    feature_data[f\"{feature}_quantile\"] = pd.qcut(feature_data[f\"{feature}\"].rank(method='first'), q=5, labels=False) + 1\n",
    "    feature_data[f\"next_{feature}_quantile\"] = pd.qcut(feature_data[f\"next_{feature}\"].rank(method='first'), q=5, labels=False) + 1\n",
    "    feature_dfs.append(feature_data.drop([f\"{feature}_quantile\", f\"next_{feature}\"], axis=1))\n",
    "\n",
    "    tickers = [ele for sub in tickers for ele in sub]\n",
    "    \n",
    "    # consolidate dfs for all features\n",
    "    feature_data = pd.concat([df for df in feature_dfs], axis=0).reset_index(drop=True)\n",
    "    feature_data[\"ticker\"] = tickers\n",
    "    feature_data[\"date_str\"] = feature_data[\"date\"].dt.strftime(\"%Y-%m-%d\")\n",
    "    feature_data[\"id\"] = feature_data[\"ticker\"] + \"-\" + feature_data[\"period\"]\n",
    "\n",
    "    final_historical = feature_data.drop([\"ticker\",\"year\", \"month\", \"day\", \"ticker\"], axis=1)\n",
    "    \n",
    "    return final_historical\n",
    "\n",
    "\n",
    "def get_sample_weights(labels):\n",
    "    \"\"\" returns array of normalized sample weight array for use in classifier.fit() \"\"\"\n",
    "    from sklearn.utils.class_weight import compute_sample_weight\n",
    "    class_weights = {}\n",
    "    unique_labels = np.unique(labels)\n",
    "    total_label_count = labels.shape[0]\n",
    "\n",
    "    # get inverted weights\n",
    "    for class_ in unique_labels:\n",
    "        label_count = labels[labels==class_].shape[0]\n",
    "        label_weight = 1 / (label_count / total_label_count)\n",
    "        class_weights[class_] = label_weight\n",
    "\n",
    "    total_class_weight = sum(class_weights.values())\n",
    "    upd_class_weights = {}\n",
    "\n",
    "    # normalize weights\n",
    "    for class_ in class_weights.keys():\n",
    "        upd_class_weights[class_] = class_weights[class_] / total_class_weight\n",
    "\n",
    "    sample_weights = compute_sample_weight(upd_class_weights, labels)\n",
    "\n",
    "    return sample_weights\n",
    "\n",
    "def dict_product(dict_):  # very cool\n",
    "    \"\"\"\n",
    "    returns generator of all key-value pairs in input dict as separate dicts\n",
    "    >>> list(dict_product(dict(number=[1,2], character='ab')))\n",
    "    [{'character': 'a', 'number': 1},\n",
    "     {'character': 'a', 'number': 2},\n",
    "     {'character': 'b', 'number': 1},\n",
    "     {'character': 'b', 'number': 2}]\n",
    "    \"\"\"\n",
    "    from itertools import product\n",
    "    return (dict(zip(dict_, x)) for x in product(*dict_.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will split out the current data from the rest of the data so that we can use that to evalute would-be performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis data has 36430 rows.\n"
     ]
    }
   ],
   "source": [
    "# Note 10Qs generally release at the end of the following month. e.g. June financials will be disseminated End of July\n",
    "\n",
    "current_period = \"2021-6\"\n",
    "equity_data[\"ev_to_ebitda\"] = np.where(equity_data[\"ev_to_ebitda\"] < 0, 100, equity_data[\"ev_to_ebitda\"])\n",
    "\n",
    "# get YoY changes for desired features and merge with original dataset on those dates/tickers\n",
    "delta_data = feature_deltas(df=equity_data, feature=\"fcf_per_share\")\n",
    "\n",
    "# get label and drop redundant columns\n",
    "delta_data[\"label\"] = np.where(delta_data[\"next_fcf_per_share_quantile\"] == 5, 1, -1)\n",
    "delta_data = delta_data.drop([\"fcf_per_share\", \"next_fcf_per_share_quantile\"], axis=1)\n",
    "\n",
    "# split out current data\n",
    "current_data = delta_data.loc[delta_data[\"period\"] == current_period]\n",
    "final_historical = delta_data.drop(current_data.index)\n",
    "\n",
    "# split out the rest of the data\n",
    "analysis_data = final_historical.merge(equity_data, left_on=\"id\", right_on=\"id\", how=\"left\").dropna()\n",
    "current_data = current_data.merge(equity_data, left_on=\"id\", right_on=\"id\", how=\"left\").dropna()\n",
    "\n",
    "# drop unneeded cols\n",
    "analysis_data = analysis_data.drop([\"date_str\", \"period_x\"], axis=1)\n",
    "current_data = current_data.drop([\"date_str\", \"period_x\"], axis=1)\n",
    "\n",
    "print(f\"Analysis data has {analysis_data.shape[0]} rows.\")\n",
    "\n",
    "premodel_data = analysis_data.drop([\"ticker\", \"period_y\", \"date\", \"date_dt\", \"id\", \"1mo_fwd_rets\", \n",
    "                                    \"1mo_fwd_log_rets\", \"1mo_log_rets\"], axis=1)\n",
    "current_data = current_data.drop([\"period_y\", \"date\", \"date_dt\", \"id\", \"1mo_fwd_rets\", \n",
    "                                    \"1mo_fwd_log_rets\", \"1mo_log_rets\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, -0.5865495470765852)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# write out current data so we can evaluate after the quarter end\n",
    "current_tickers = current_data.loc[current_data[\"label\"] == 1]\n",
    "current_tickers.to_csv(\"current_tickers.csv\")\n",
    "len(current_tickers), premodel_data[\"label\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets define our Premodel class that will allow us to do various premodeling stops including splitting into train and test data, running SKLearn grid searches, and examine feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Premodel:\n",
    "    plt.rcParams['figure.figsize'] = (8.0, 5.0)\n",
    "\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    def train_test_split(self, label, train_size=.8):\n",
    "        \"\"\" \n",
    "        Split processed data into train and test sets. Extract labels. Also stores indexed labels and data in the process.\n",
    "        Data is required to have the label_col column that represents train and test labels.\n",
    "        \"\"\"\n",
    "        # Carve out holdout data\n",
    "        holdout = self.data.sample(frac=.2)\n",
    "        non_holdout_data = self.data.drop(holdout.index, axis=0)\n",
    "        holdout_labels = holdout[label] \n",
    "        holdout_data = holdout.drop([label], axis=1)\n",
    "\n",
    "        # split out train and test from non-holdout data\n",
    "        labeled_train_data = non_holdout_data.sample(frac=.8)\n",
    "        labeled_test_data = non_holdout_data.drop(labeled_train_data.index)\n",
    "\n",
    "        train_data = labeled_train_data.drop([label], axis=1).reset_index(drop=True)\n",
    "        train_labels = labeled_train_data[label].reset_index(drop=True)\n",
    "\n",
    "        test_data = labeled_test_data.drop([label], axis=1).reset_index(drop=True)\n",
    "        test_labels = labeled_test_data[label].reset_index(drop=True)\n",
    "\n",
    "        # normalize data\n",
    "        scaler = StandardScaler()\n",
    "        normed_train_data = scaler.fit_transform(train_data)\n",
    "        normed_test_data = scaler.transform(test_data)\n",
    "        normed_holdout_data = scaler.transform(holdout_data)\n",
    "        \n",
    "        normed_train_data = pd.DataFrame(data=normed_train_data, columns=[c for c in self.data.columns if c != \"label\"])\n",
    "        assert(\"label\" not in normed_train_data.columns)\n",
    "\n",
    "        normed_test_data = pd.DataFrame(data=normed_test_data, columns=[c for c in self.data.columns if c != \"label\"])\n",
    "        \n",
    "        normed_holdout_data = pd.DataFrame(data=normed_holdout_data, columns=[c for c in self.data.columns if c != \"label\"])\n",
    "\n",
    "        return normed_train_data, normed_test_data, normed_holdout_data, train_labels, test_labels, holdout_labels\n",
    "\n",
    "    def pca_transform(self, train_data, test_data, n_components):\n",
    "        \"\"\" take train data and apply PCA. We do this so we always train on the same records \"\"\"\n",
    "        pca = PCA(n_components)\n",
    "        pca_train_data = pca.fit_transform(train_data)\n",
    "        pca_test_data = pca.fit_transform(test_data)\n",
    "\n",
    "        return pca_train_data, pca_test_data, pca\n",
    "    \n",
    "\n",
    "    def run_grid(self, clf, grid, train_data, test_data, train_labels, test_labels, n_jobs=-1, score=\"accuracy\"):\n",
    "        \"\"\" \n",
    "        Runs SKLearn implementation of grid search \n",
    "        clf: un-fit SKLearn classifier\n",
    "        grid: params upon which grid search will run\n",
    "        data: pandas DataFrame of model data, inclusive of label\n",
    "        label: name of column that holds classification labels\n",
    "\n",
    "        returns: Most accurate model\n",
    "        \"\"\"\n",
    "        grid_search = GridSearchCV(estimator=clf, param_grid=grid, n_jobs=n_jobs, scoring=score)\n",
    "        best_clf = grid_search.fit(train_data, train_labels).best_estimator_\n",
    "\n",
    "        # get model predictions and accuracy\n",
    "        preds = best_clf.predict(test_data)\n",
    "        acc = metrics.accuracy_score(preds, test_labels)\n",
    "\n",
    "        print(f\"Model Accuracy: {acc*100:.1f}%\")\n",
    "\n",
    "        return best_clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring Interactions\n",
    "\n",
    "Before moving onto feature selection, we are going to take a look at interactions between variables in our feature space. Once we are done examining features, we will keep the ones with the largest difference between feature quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_interaction(df, x, x2, label):\n",
    "    \"\"\"\n",
    "    Plots the probability of increase of the label at the cross-sectional performance of variables x and x2\n",
    "    df: pandas dataframe containing columns with independent vars and labels\n",
    "    x: column name of an independent variable within the dataframe\n",
    "    x2: column name of a second independent variable within the dataframe\n",
    "    label: column name of the label in which we are concerned\n",
    "    x_buckets: number of buckets into which we split the primary x variable \n",
    "    x2_buckets: number of buckets into which we split the secondary x variable \n",
    "    \n",
    "    returns plot and constructed dataframe\n",
    "    \"\"\"\n",
    "    plt.rcParams['figure.figsize'] = (10.0, 6.0)\n",
    "    df2 = df.copy()\n",
    "    # get quantiles of variables\n",
    "    df2[f\"{x}_quantile\"] = pd.qcut(df2[x].rank(method='first'), q=4, labels=False) + 1\n",
    "    df2[f\"{x2}_quantile\"] = pd.qcut(df2[x2].rank(method='first'), q=3, labels=False) + 1\n",
    "\n",
    "    # bucket into high, med, low groups based on x2\n",
    "    high_x2 = df2.loc[df2[f\"{x2}_quantile\"] == 3]\n",
    "    mid_x2 = df2.loc[df2[f\"{x2}_quantile\"] == 2]\n",
    "    low_x2 = df2.loc[df2[f\"{x2}_quantile\"] == 1]\n",
    "\n",
    "    # get probs of increase given high x2\n",
    "    p_increase_very_high_x_high_x2 = high_x2.loc[high_x2[f\"{x}_quantile\"] == 4][label].value_counts().loc[1] / \\\n",
    "                                    high_x2.loc[high_x2[f\"{x}_quantile\"] == 4].shape[0]\n",
    "    p_increase_high_x_high_x2 = high_x2.loc[high_x2[f\"{x}_quantile\"] == 3][label].value_counts().loc[1] / \\\n",
    "                                    high_x2.loc[high_x2[f\"{x}_quantile\"] == 3].shape[0]\n",
    "    p_increase_low_x_high_x2 = high_x2.loc[high_x2[f\"{x}_quantile\"] == 2][label].value_counts().loc[1] / \\\n",
    "                                    high_x2.loc[high_x2[f\"{x}_quantile\"] == 2].shape[0]\n",
    "    p_increase_very_low_x_high_x2 = high_x2.loc[high_x2[f\"{x}_quantile\"] == 1][label].value_counts().loc[1] / \\\n",
    "                                    high_x2.loc[high_x2[f\"{x}_quantile\"] == 1].shape[0]\n",
    "    high_x2_data = [p_increase_very_low_x_high_x2, p_increase_low_x_high_x2, p_increase_high_x_high_x2, p_increase_very_high_x_high_x2]\n",
    "\n",
    "    # get probs of increase given mid x2\n",
    "    p_increase_very_high_x_mid_x2 = mid_x2.loc[mid_x2[f\"{x}_quantile\"] == 4][label].value_counts().loc[1] / \\\n",
    "                                    mid_x2.loc[mid_x2[f\"{x}_quantile\"] == 4].shape[0]\n",
    "    p_increase_high_x_mid_x2 = mid_x2.loc[mid_x2[f\"{x}_quantile\"] == 3][label].value_counts().loc[1] / \\\n",
    "                                    mid_x2.loc[mid_x2[f\"{x}_quantile\"] == 3].shape[0]\n",
    "    p_increase_low_x_mid_x2 = mid_x2.loc[mid_x2[f\"{x}_quantile\"] == 2][label].value_counts().loc[1] / \\\n",
    "                                    mid_x2.loc[mid_x2[f\"{x}_quantile\"] == 2].shape[0]\n",
    "    p_increase_very_low_x_mid_x2 = mid_x2.loc[mid_x2[f\"{x}_quantile\"] == 1][label].value_counts().loc[1] / \\\n",
    "                                    mid_x2.loc[mid_x2[f\"{x}_quantile\"] == 1].shape[0]\n",
    "    mid_x2_data = [p_increase_very_low_x_mid_x2, p_increase_low_x_mid_x2, p_increase_high_x_mid_x2, p_increase_very_high_x_mid_x2]\n",
    "\n",
    "    # xs given low y\n",
    "    p_increase_very_high_x_low_x2 = low_x2.loc[low_x2[f\"{x}_quantile\"] == 4][label].value_counts().loc[1] / \\\n",
    "                                    low_x2.loc[low_x2[f\"{x}_quantile\"] == 4].shape[0]\n",
    "    p_increase_high_x_low_x2 = low_x2.loc[low_x2[f\"{x}_quantile\"] == 3][label].value_counts().loc[1] / \\\n",
    "                                    low_x2.loc[low_x2[f\"{x}_quantile\"] == 3].shape[0]\n",
    "    p_increase_low_x_low_x2 = low_x2.loc[low_x2[f\"{x}_quantile\"] == 2][label].value_counts().loc[1] / \\\n",
    "                                    low_x2.loc[low_x2[f\"{x}_quantile\"] == 2].shape[0]\n",
    "    p_increase_very_low_x_low_x2 = low_x2.loc[low_x2[f\"{x}_quantile\"] == 1][label].value_counts().loc[1] / \\\n",
    "                                    low_x2.loc[low_x2[f\"{x}_quantile\"] == 1].shape[0]\n",
    "    low_x2_data = [p_increase_very_low_x_low_x2, p_increase_low_x_low_x2, p_increase_high_x_low_x2, p_increase_very_high_x_low_x2]\n",
    "\n",
    "    # get probs of increase given low x2\n",
    "    data = high_x2_data + mid_x2_data + low_x2_data\n",
    "    y_labels = [f\"high {x2}\"] * 4 + [f\"mid {x2}\"] * 4 + [f\"low {x2}\"]*4\n",
    "    x_labels = [\"Very low\", \"Low\", \"High\", \"Very high\"] * 3\n",
    "\n",
    "    tmp = pd.DataFrame(data, columns=[\"data\"])\n",
    "    tmp[\"y_labels\"] = y_labels\n",
    "    tmp[\"x_labels\"] = x_labels\n",
    "\n",
    "    ### PLOTTING\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    # aesthetics\n",
    "    sns.set_style(\"whitegrid\");\n",
    "    palette = sns.color_palette([\"#55a868\", \"#4c72b0\", \"#c44e55\"])\n",
    "\n",
    "    # Create Labels\n",
    "    plt.ylabel(\"Probability\")\n",
    "    plt.xlabel(x.replace(\"_\", \" \").upper())\n",
    "    plt.title(f\"Prob. of positive change in {label.upper()} given {x.replace('_', ' ').upper()} and {x2.upper()}\")\n",
    "\n",
    "    # Plot\n",
    "    g = sns.lineplot(data=tmp, y=\"data\", x=\"x_labels\", hue=\"y_labels\", ax=ax, palette=palette)\n",
    "    ax.legend([f\"High\", f\"Medium\", f\"Low\"]).set_title(f\"{x2.replace('_', ' ').upper()} Label\")\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "def measure_interactions(df, x, x2, label, x_buckets=3, x2_buckets=4, threshold=100):\n",
    "    \"\"\" \n",
    "    Measures the difference between interactions according to the predictive power of various buckets\n",
    "    df: pandas DataFrame containing columns x, x2, and label\n",
    "    x: name of first variable whose interaction we are interested in\n",
    "    x2: name of second variable whose interaction we are interested in\n",
    "    label: name of label column we will use to judge interactions\n",
    "    x_buckets: number of buckets used to measure x\n",
    "    x2_buckets: number of buckets used to measure x2\n",
    "    \n",
    "    returns: sum of differences between buckets and those sequentially lower (for those which it applies)\n",
    "    --> e.g. buckets 3-2, 3-1, and 2-1 \n",
    "    \"\"\"\n",
    "    quantile_data = {}  # store percent of positive labels for each intersection\n",
    "    df2 = df.copy()\n",
    "\n",
    "    # get quantiles of x and x2 variables\n",
    "    df2[f\"{x}_quantile\"] = pd.qcut(df2[x].rank(method='first'), q=x_buckets, labels=False) + 1\n",
    "    df2[f\"{x2}_quantile\"] = pd.qcut(df2[x2].rank(method='first'), q=x2_buckets, labels=False) + 1\n",
    "    q_data = df2[[f\"{x}_quantile\", f\"{x2}_quantile\", label]]\n",
    "\n",
    "    for i in range(1, x_buckets+1):\n",
    "        # split data into buckets along x1 variable\n",
    "        i_data = q_data.loc[q_data[f\"{x}_quantile\"] == i]\n",
    "\n",
    "        for j in range(1, x2_buckets+1):\n",
    "            # split x1 bucket data according to x2 buckets\n",
    "            j_data = i_data.loc[i_data[f\"{x2}_quantile\"] == j]\n",
    "            try:\n",
    "                positive_labels = j_data[label].value_counts()[1]\n",
    "                negative_labels = j_data[label].value_counts()[-1]\n",
    "\n",
    "                # if not enough observations, skip over\n",
    "                if (positive_labels < threshold) or (negative_labels < threshold):\n",
    "                    pass\n",
    "                else:\n",
    "                    quantile_data[j] = positive_labels / negative_labels \n",
    "\n",
    "            except KeyError as e: \n",
    "#                 e.g. all 1000 observations of high \"x\" correspond to low/medium \"x2\"\n",
    "#                 print(f\"{x} and {x2} showing only negative labels\")  \n",
    "                pass  \n",
    "\n",
    "    # get differences (H-M, H-L, and so on)\n",
    "    diffs = []\n",
    "\n",
    "    # only examine the relationship if we have enough obs at each quantile\n",
    "    if len(quantile_data.keys()) < x2_buckets:\n",
    "        pass\n",
    "    else: \n",
    "        for n in list(quantile_data.keys())[::-1]:\n",
    "            if n > 1:\n",
    "                # for each quantile, get individual values by subtracting all lower quantiles\n",
    "                try:\n",
    "                    for n2 in range(1, n)[::-1]:\n",
    "                        diffs.append(quantile_data[n] - quantile_data[n-n2])\n",
    "\n",
    "                except KeyError as e:\n",
    "                    pass\n",
    "                \n",
    "    return sum(diffs)\n",
    "\n",
    "def add_interactions(df, col_names):\n",
    "    \"\"\" \n",
    "    Calculate interactions between specified columns\n",
    "    df: pandas DataFrame with input data\n",
    "    col_names: columns whose interactions we are interested in calculating\n",
    "    \n",
    "    returns: pandas DataFrame containing interaction calculations\n",
    "    \"\"\"\n",
    "    # get column names corresponding to the top n ineractions -- should be 2n\n",
    "    # get individual factors into flat list\n",
    "    col_names_stacked = [x.split(\"-\") for x in col_names]\n",
    "    col_names_flat = [ele for sublist in col_names_stacked for ele in sublist]\n",
    "\n",
    "    # filter dataframe on just the columns in which we are interested\n",
    "    interaction_data = df[col_names_flat]\n",
    "\n",
    "    # get final list of columns\n",
    "    combos = list(combinations(list(interaction_data.columns), 2))\n",
    "    col_names_final = list(interaction_data.columns) + ['-'.join(x) for x in combos]\n",
    "\n",
    "    # calc interactions\n",
    "    poly = PolynomialFeatures(interaction_only=True, include_bias=False)\n",
    "    tmp = poly.fit_transform(interaction_data)\n",
    "\n",
    "    # filter to just the data we are interested in\n",
    "    interaction_data = pd.DataFrame(tmp, columns=col_names_final)\n",
    "    interaction_data = interaction_data[col_names]\n",
    "    non_dupe_data = interaction_data.loc[:,~interaction_data.columns.duplicated()]\n",
    "    \n",
    "    return non_dupe_data.merge(df, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the functions are defined to examine the interactions, lets run our data through them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['book_value_per_share-price_to_book_value',\n",
       " 'capital_expenditure-earnings_per_diluted_share',\n",
       " 'capital_expenditure-earnings_per_basic_share',\n",
       " 'price_to_book_value-tangible_assets_book_value_per_share',\n",
       " 'profit_margin-sales_per_share']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at the top interactions according to differences across buckets\n",
    "interactions = {}\n",
    "\n",
    "# get all combos and iterate through \n",
    "combos = list(combinations(list(premodel_data.drop([\"label\"], axis=1).columns), 2))\n",
    "for combo in combos:\n",
    "    x = combo[0]\n",
    "    x2 = combo[1]\n",
    "    \n",
    "    # for each combination of features, measure the interaction\n",
    "    interactions[f\"{x}-{x2}\"]  = measure_interactions(premodel_data.copy(deep=True), \n",
    "                                                      x=x, x2=x2, label=\"label\", x_buckets=3)\n",
    "    \n",
    "# sort dict by values to we can take the interactions with the most extreme differences.\n",
    "interactions = list({k: v for k, v in sorted(interactions.items(), key=lambda item: item[1])}.keys())\n",
    "\n",
    "# arbitrary: we're going to keep top 10 interaction\n",
    "bot = interactions[:5]\n",
    "top = interactions[-5:]\n",
    "\n",
    "# add interactions to our data according to largest differences\n",
    "int_premodel_data = add_interactions(premodel_data, top+bot)\n",
    "top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize a top interaction\n",
    "plot_interaction(int_premodel_data, x=\"current_assets\", x2=\"sales_per_share\", label=\"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance\n",
    "\n",
    "The method we will use to detect feature importance is mean decrease accuracy (MDA), which is a form of backward selection.\n",
    "We will start by defining a class that will have methods allowing us to robustly determine feature importance according to MDA.\n",
    "\n",
    "For more info on stepwise feature selection: https://en.wikipedia.org/wiki/Stepwise_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureImportance:\n",
    "    def __init__(self, clf, train_data, train_labels):\n",
    "        self.clf = clf\n",
    "        self.data = train_data\n",
    "        self.labels = train_labels\n",
    "        self.sample_weights = get_sample_weights(train_labels)\n",
    "        \n",
    "    def cv_score(self, data, labels, sample_weights, n_splits=5, score_type=\"neg_log_loss\", all_times=None, cv=None):\n",
    "        \"\"\" \n",
    "        Implement CV to handle current bugs in SKlearn (scoring w classes). \n",
    "        \"\"\"\n",
    "        if score_type not in [\"neg_log_loss\", \"accuracy\"]:\n",
    "            raise Exception(\"Scoring method not allowed. Try neg_log_loss or accuracy\")\n",
    "\n",
    "        from sklearn.metrics import log_loss, accuracy_score\n",
    "        scores = []\n",
    "\n",
    "        if cv is None:\n",
    "            from sklearn.model_selection import KFold\n",
    "            cv = KFold(n_splits=n_splits)\n",
    "\n",
    "        for train, test in cv.split(data, labels):  # use sklearn generator for KFold CV\n",
    "            fit = self.clf.fit(data.iloc[train], labels.iloc[train], sample_weight=sample_weights[train])  # no iloc for weights b/c it's an array\n",
    "\n",
    "            if score_type == \"neg_log_loss\":\n",
    "                probs = fit.predict_proba(data.iloc[test])\n",
    "                score_ = -log_loss(labels.iloc[test], probs, sample_weight=sample_weights[test], labels=clf.classes_)  # classes_?\n",
    "\n",
    "            else:\n",
    "                preds = fit.predict(data.iloc[test])\n",
    "                score_ = accuracy_score(labels.iloc[test], preds, sample_weight=sample_weights[test])\n",
    "\n",
    "            scores.append(score_)\n",
    "\n",
    "        return np.array(scores)\n",
    "    \n",
    "    def feature_mda(self, clf, data, labels, sample_weights, cv=None, n_splits=5, score_type=\"neg_log_loss\"):\n",
    "        \"\"\" \n",
    "        OOS feature importance. Similar to the cv_score above. Can use purged or un-purged (bootstrapped) self.data \n",
    "        Note that unlike MDI, MDA requires an unfit classifier. Returns DF of mean impurity and std for each feature, \n",
    "        as well as the mean train score. Each feature's importance is a function of its loss in performance caused by its columns permutation.\n",
    "        Note: this can usefully be applied to meta-labeling by specifying 'f1' as the score_type\n",
    "        \"\"\"\n",
    "        # feature importance based on OOS score reduction. B/c it is OOS-based, it is possible that MDA concludes that all features are unimportant\n",
    "        if score_type not in [\"neg_log_loss\", \"accuracy\", \"f1\"]:\n",
    "            raise Exception(\"Scoring method not allowed. Try neg_log_loss or accuracy\")\n",
    "\n",
    "        from sklearn.metrics import log_loss, accuracy_score\n",
    "        if cv is None:\n",
    "            from sklearn.model_selection import KFold\n",
    "            cv = KFold(n_splits=n_splits)\n",
    "\n",
    "        scores0 = pd.Series(dtype=np.float64)\n",
    "        scores1 = pd.DataFrame(columns=data.columns)\n",
    "        for i, (train, test) in enumerate(cv.split(data, labels)):      \n",
    "            # first: get splits and fit classifier\n",
    "            x0, y0, w0 = data.iloc[train], labels.iloc[train], sample_weights[train]  # remember, self.sample_weights is an array\n",
    "            x1, y1, w1 = data.iloc[test], labels.iloc[test], sample_weights[test]\n",
    "            fit = clf.fit(x0, y0, sample_weight=w0)\n",
    "\n",
    "            # second: get performance scores (and store)\n",
    "            if score_type == \"neg_log_loss\":\n",
    "                probs = fit.predict_proba(data.iloc[test])\n",
    "                scores0.loc[i] = -log_loss(labels.iloc[test], probs, sample_weight=sample_weights[test], labels=clf.classes_) \n",
    "\n",
    "            else:\n",
    "                preds = fit.predict(data.iloc[test])\n",
    "                scores0.loc[i] = accuracy_score(labels.iloc[test], preds, sample_weight=sample_weights[test])\n",
    "\n",
    "            # shuffle (permutate) each column of the feature matrix one at a time to derive the OOS performance after each permutation\n",
    "            for j in scores1.columns:\n",
    "                x1_ = x1.copy(deep=True)\n",
    "                np.random.shuffle(x1_[j].values)  # get permutation of a single col. np must align indicies automatically.\n",
    "\n",
    "                if score_type == \"neg_log_loss\":\n",
    "                    probs = fit.predict_proba(x1_)\n",
    "                    scores1.loc[i, j] = -log_loss(y1, probs, sample_weight=w1, labels=clf.classes_)  \n",
    "\n",
    "                else:\n",
    "                    preds = fit.predict(x1_)\n",
    "                    scores1.loc[i, j] = accuracy_score(y1, preds, sample_weight=w1)\n",
    "\n",
    "        importance = (-scores1).add(scores0, axis=0) \n",
    "        if score_type == \"neg_log_loss\":\n",
    "            importance /= -scores1\n",
    "        else:\n",
    "            importance /= (1.0 - scores1)\n",
    "\n",
    "        importance_std = importance.std()*importance.shape[0]**-.5\n",
    "        importance = pd.concat({\"imp_mean\": importance.mean(), \"imp_std\": importance_std}, axis=1)\n",
    "\n",
    "        return importance.sort_values(\"imp_mean\", ascending=False), scores0.mean()\n",
    "\n",
    "\n",
    "    def _get_eigen_vec(self, dot, var_threshold):  \n",
    "        \"\"\" \n",
    "        compute eigenvector of a feature matrix from the dot product matrix, reduce dimension \n",
    "        dot: dot products of feature space\n",
    "        var_threshold: threshold of cumulative variance we wish to capture\n",
    "        \n",
    "        returns: eigenvalues and eigenvector\n",
    "        \"\"\"\n",
    "        e_val, e_vec = np.linalg.eigh(dot) # dict as mapping will work as long as these dont shuffle\n",
    "\n",
    "        idx = e_val.argsort()[::-1] \n",
    "        e_val, e_vec = e_val[idx], e_vec[:, idx]\n",
    "\n",
    "        # # only positive eigenvalues\n",
    "        e_val = pd.Series(e_val, index=[i for i in range(e_val.shape[0])])  \n",
    "        e_vec = pd.DataFrame(data=e_vec, index=dot.index, columns=e_val.index)\n",
    "        e_vec = e_vec.loc[:, e_val.index]\n",
    "\n",
    "        # reduce dimension, form principal components\n",
    "        cum_var = e_val.cumsum() / e_val.sum() \n",
    "        dim = cum_var.values.searchsorted(var_threshold)   # error here. shape from 110 (expected) to (10000, 40)\n",
    "        e_val = e_val.iloc[:dim+1]\n",
    "        e_vec = e_vec.iloc[:, :dim+1]\n",
    "\n",
    "        return e_val, e_vec\n",
    "\n",
    "\n",
    "    def get_ortho_features(self, data, var_threshold=.95):  \n",
    "        \"\"\" \n",
    "        given self.dataframe of features, compute array of orthogonal features that \"explain\" >= var_threshold of the variance.\n",
    "\n",
    "        'Diagonalization' performed on normed self.data for two reasons: \n",
    "        1. centering the self.data ensures that the first PC is correctly oriented in the main \n",
    "        direction of the observations\n",
    "        2. re-scaling the self.data makes PCA focus on explaining correlations rather than variances; w/o\n",
    "        re-scaling, the first PCs would be dominated by the columns of the non-normed with the highest var,\n",
    "        meaning we would not learn much about the structure of, nor the relationship between the variables.\n",
    "        Here we are trying to learn about the relationship between vars. (would we ever want to focus on variance?)\n",
    "        Remember, this helps handle subsitution effects.\n",
    "\n",
    "        data: dataset for which we will calculate orthogonal features. not necessarily self.data (e.g. in the case of self.get_baseline())\n",
    "\n",
    "        \"\"\"        \n",
    "        # 'diagonalize' so we can get eigens with helper function get_eigen_vec()\n",
    "        dot = pd.DataFrame(np.dot(data.T, data), columns=data.columns, index=data.columns)  \n",
    "        idx_map = dict(enumerate(dot.columns))\n",
    "        e_val, e_vec = self._get_eigen_vec(dot=dot, var_threshold=var_threshold)\n",
    "\n",
    "        ortho_features = pd.DataFrame(np.dot(data, e_vec))\n",
    "        cols = [idx_map[col]+\"_O\" for col in ortho_features.columns]\n",
    "        ortho_features.columns = cols\n",
    "\n",
    "        return ortho_features\n",
    "\n",
    "\n",
    "    def get_feat_correlation(self, ortho_feats, feat_importance):\n",
    "        \"\"\" \n",
    "        get correlation between feature importances and their inverse pca rank \n",
    "        note that inverse pca rank corresponds to high eigenvalues\n",
    "        \"\"\"\n",
    "        from scipy.stats import weightedtau\n",
    "        pcs = [i for i in ortho_feats.columns]\n",
    "        pc_importances = feat_importance.T[pcs].loc[\"imp_mean\"]\n",
    "        pc_ranks = ortho_feats.mean(axis=0).rank()\n",
    "\n",
    "        return weightedtau(pc_importances, pc_ranks**-1.)  \n",
    "\n",
    "\n",
    "    def synthetic_dataset(self, n_features=40, n_informative=10, n_redundant=10, n_samples=10000, seed=None):\n",
    "        \"\"\" \n",
    "        generate a self.dataset of n_samples, which includes informative features, redundant features, and noise \n",
    "        returns equally-weighted sample weights\n",
    "        \"\"\" \n",
    "        from sklearn.datasets import make_classification\n",
    "        if seed is None:\n",
    "            seed = np.random.randint(0, 1000)\n",
    "        synth_data, synth_labels = make_classification(n_samples=n_samples, n_features=n_features, n_informative=n_informative, \n",
    "                                           n_redundant=n_redundant, random_state=seed, shuffle=False)\n",
    "\n",
    "        synth_data = pd.DataFrame(synth_data, index=np.arange(0, n_samples))\n",
    "        synth_labels = pd.Series(synth_labels, index=np.arange(0, n_samples)).to_frame(name=\"label\")  # converts to df via built-in\n",
    "        n_noise = n_features - n_informative - n_redundant\n",
    "\n",
    "        df_cols = [\"I_\"+str(i) for i in range(n_informative)] + [\"R_\"+str(i) for i in range(n_redundant)] + \\\n",
    "        [\"N_\"+str(i) for i in range(n_noise)]  # fill rest with noise\n",
    "        synth_data.columns = df_cols\n",
    "        synth_labels[\"sample_weights\"] = 1. / synth_labels.shape[0]  # equal \n",
    "\n",
    "        print(f\"Synthetic self.dataset created with seed {seed}\")\n",
    "\n",
    "        return synth_data, synth_labels\n",
    "\n",
    "\n",
    "    def test_feature_importance(self, data, labels, sample_weights, n_estimators=100, cv=5, max_samples=1., n_jobs=-1, \n",
    "                                score_type=[\"accuracy\"], method=\"MDI\", min_weight_fraction_leaf=0., **kwargs):\n",
    "        \"\"\" use to test feature importance measures with random forest \"\"\"\n",
    "        if n_jobs == -1:\n",
    "            n_jobs = os.cpu_count()\n",
    "\n",
    "        clf = self.clf(**kwargs, n_jobs=-1)\n",
    "        fit = clf.fit(data, labels, sample_weight=sample_weights)\n",
    "        try:\n",
    "            oob = fit.oob_score_\n",
    "        except AttributeError as e:  # will throw error if clf is not a tree-based estimator\n",
    "            oob = None\n",
    "\n",
    "        if method == \"MDA\":\n",
    "            imp, oos = self.feature_mda(clf, data, labels, sample_weights, score_type=score_type)\n",
    "            \n",
    "        elif method == \"MDI\":\n",
    "            raise NotImplementedError\n",
    "            imp = self.feature_mdi(fit, data.columns)\n",
    "            oos = self.cv_score(score_type=score_type).mean()\n",
    "\n",
    "        elif method == \"SFI\":\n",
    "            raise NotImplementedError\n",
    "            imp = self.single_feature_importance(data, labels, sample_weights) \n",
    "            oos = self.cv_score(score_type=score_type).mean()\n",
    "\n",
    "        return imp, oob, oos\n",
    "\n",
    "\n",
    "    def feature_test(self, data, labels, sample_weights, param_grid, n_features=40, n_informative=10, n_redundant=10, \n",
    "                     score_type=[\"neg_log_loss\"], method=[\"MDA\", \"MDI\", \"SFI\"], n_samples=10000, cv=5, pca=False, baseline=False):\n",
    "        \"\"\" \n",
    "        function that calls all components of feature importance testing (self.data generation, FI analysis, etc.) \n",
    "        and processes output such that feature importances and scores are aggregated for each importance method.\n",
    "        If pca is True, the features are 'orthogonalized' based on variance\n",
    "\n",
    "        Processing requires running cv'ed test_feature_importance on each scoring method (MDI, MDA, SFI)\n",
    "        returns: self.dataframe of feature importance for each feature type, as well as the importance method,\n",
    "        scoring method, and any other kwargs passed. Index simply represents nth iteration of the mesh \n",
    "\n",
    "        Output:\n",
    "        \"\"\"\n",
    "        if pca:\n",
    "            data = self.get_ortho_features(data)\n",
    "\n",
    "        # construct param grid for feature importance testing\n",
    "        param_grid[\"score_type\"] = score_type\n",
    "        param_grid[\"method\"] = method\n",
    "        param_mesh = dict_product(param_grid)\n",
    "        output = []\n",
    "\n",
    "        for kwargs in param_mesh:\n",
    "            imp, oob, oos = self.test_feature_importance(data=data, labels=labels, sample_weights=sample_weights, \n",
    "                                                         cv=cv, **kwargs)\n",
    "            df0 = imp[[\"imp_mean\"]] / imp[\"imp_mean\"].abs().sum()\n",
    "            if baseline:\n",
    "                df0[\"type\"] = [i[0] for i in df0.index]  # index should be the features -- e.g. R_1\n",
    "                df0 = df0.groupby(\"type\")[\"imp_mean\"].sum().to_dict()  # get feature importance by feature type\n",
    "            else:\n",
    "                df0[\"type\"] = [i for i in df0.index]  # index should be the features -- e.g. R_1\n",
    "                df0 = df0.groupby(\"type\")[\"imp_mean\"].sum().to_dict()  # get feature importance by feature type\n",
    "            df0.update({\"oob\": oob, \"oos\": oos}, **kwargs)  # i am such an animal\n",
    "            output.append(df0)\n",
    "\n",
    "        output = pd.DataFrame(output).sort_values([\"method\", \"score_type\"])\n",
    "        output = output.loc[:, ~output.columns.duplicated()]  # method and score_type would be duped\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "    def get_baseline(self, param_grid, score_type=[\"neg_log_loss\"], n_samples=10000):\n",
    "        \"\"\"\n",
    "        use synthetic data to get baseline feature importance measures.\n",
    "        param_grid: sklearn grid_search object used for clf parameters\n",
    "\n",
    "        returns: feature importance summary of synthetic data\n",
    "         \"\"\"\n",
    "        synth_data, synth_labels = self.synthetic_dataset(n_samples=n_samples)\n",
    "        synth_sample_weights = synth_labels[\"sample_weights\"]\n",
    "        synth_labels = synth_labels[\"label\"]\n",
    "        ortho_synth_data = self.get_ortho_features(synth_data)  # error here\n",
    "        ortho_synth_output = self.feature_test(data=ortho_synth_data, labels=synth_labels, sample_weights=synth_sample_weights,\n",
    "            param_grid=param_grid, method=[\"MDA\"], baseline=True, score_type=score_type)\n",
    "\n",
    "        combined_synth_feature_data = synth_data.join(ortho_synth_data)\n",
    "        combined_synth_output = self.feature_test(data=combined_synth_feature_data, labels=synth_labels,sample_weights=synth_sample_weights,\n",
    "            param_grid=param_grid, method=[\"MDA\"], baseline=True,score_type=score_type)\n",
    "\n",
    "        return ortho_synth_output, combined_synth_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MDA\n",
    "\n",
    "Now that the functions are defined, we will run an SKLearn grid search to figure out which model performs best. Then we will run through our FeatureImportance class to discover the most important features and the values we assigned to them. \n",
    "\n",
    "We will start with a baseline of synthetic data so we can use that as a point of comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tol = [1e-5, 1e-4]\n",
    "\n",
    "# Inverse regularization strength -- smaller number means greater regularization\n",
    "C = [.05, .25, .5, .75, 1.0]\n",
    "\n",
    "lasso_grid = { \n",
    "    'tol': tol, \n",
    "    'C': C,\n",
    "    \"solver\": [\"saga\"]}\n",
    "\n",
    "lasso_clf = LogisticRegression\n",
    "\n",
    "# setup premodel class\n",
    "pm = Premodel(data=int_premodel_data)\n",
    "\n",
    "# split train, test, and holdout data along with sample weights\n",
    "train_data, test_data, holdout_data, train_labels, test_labels, holdout_labels = pm.train_test_split(label=\"label\")\n",
    "train_sample_weights = get_sample_weights(train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare our training data for running through our feature importance pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic self.dataset created with seed 979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\lbianculli\\anaconda3\\envs\\env3.7\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# setup feature importance class\n",
    "feat_imp = FeatureImportance(lasso_clf, train_data, train_labels)\n",
    "\n",
    "# get baseline results\n",
    "ortho_baseline, combined_ortho_baseline = feat_imp.get_baseline(lasso_grid, score_type=[\"neg_log_loss\"])\n",
    "\n",
    "# run feature importance with train data\n",
    "train_output = feat_imp.feature_test(param_grid=lasso_grid, data=train_data, labels=train_labels, \n",
    "            sample_weights=train_sample_weights, method=[\"MDA\"], score_type=[\"neg_log_loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14, 1)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the average MDA for noise variables. We will use this to filter our feature set later\n",
    "redundant_mda = combined_ortho_baseline[\"R\"].min()\n",
    "\n",
    "# keep only the features and get mean importance scores across top clfs. We will use just the train output\n",
    "best_feat_scores = train_output.sort_values(\"oos\", ascending=False).iloc[0:5].drop([\"C\", \"oos\", \"oob\", \"tol\", \"solver\", \"score_type\", \"method\"], axis=1)\n",
    "best_feat_scores = pd.DataFrame(best_feat_scores.mean(), columns=[\"imp\"]).sort_values(\"imp\", ascending=False)\n",
    "\n",
    "# reduce data set to features that are important, where important means greater than mean importance\n",
    "useful_features = best_feat_scores.loc[abs(best_feat_scores[\"imp\"]) > redundant_mda]\n",
    "\n",
    "# get final data with only the features we deemed important\n",
    "model_data = int_premodel_data[[\"label\"] + [feat for feat in useful_features.index]]\n",
    "useful_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are left with 14 features deemed useful according to MDA. We will use those as our feature space for training our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write out new data for classifier training\n",
    "model_data.to_csv(\"C:/Users/lbianculli/equity_analysis/model_data_final.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now that premodeling is done, we move onto model selection"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
